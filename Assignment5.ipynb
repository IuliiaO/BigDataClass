{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1**. (100 points)\n",
    "\n",
    "In this exercise you will use Spark to build and run a machine learning pipeline to separate 'ham' from 'spam' in SMS text messages. Then you will use the pipeline to classify SMS texts.\n",
    "\n",
    "- Create a Pandas DataFraem form the data in the file`SMSSpamCollection` where each line is tab separated into (label, text). If you find that the read_xxx function in Pandas does not do the job correctly, read in the file line by line before converting to a DataFrame. Create an index column so that each row has a unique number id.\n",
    "- Convert to a Spark DataFrame that has two columns (klass, SMS) and split into test and training data sets with proportions 0.8 and 0.2 respectively using a random seed of 123.\n",
    "- Build a Spark ML pipeline consisting of the following \n",
    "    - StringIndexer: To convert `klass` into a numeric `labels` column\n",
    "    - Tokenizer: To covert `SMS` into a list of tokens\n",
    "    - StopWordsRemover: To remove \"stop words\" from the tokens\n",
    "    - CountVectorizer: To count words (use a vocabular size of 100 and minimum number of occureences of 2)\n",
    "    - LogisticRegression: Use `maxIter=10`, `regParam=0.001`\n",
    "\n",
    "- Train the model on the test data.\n",
    "- Evaluate the precision, recall and accuracy of this model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/iuliia/bios-823-2019/homework/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "with open('SMSSpamCollection') as f:\n",
    "    content = f.readlines()\n",
    "content = [x.strip() for x in content] #remove whitespace characters like `\\n` at the end of each line\n",
    "data=pd.DataFrame(content, columns=['text'])\n",
    "data = pd.DataFrame(data.text.str.split('\\t',1).tolist(), columns = ['klass','SMS']) #Separating each line by \\t into two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not spam: 4827\n",
      "Spam: 747\n"
     ]
    }
   ],
   "source": [
    "print ('Not spam:', len(data[data['klass']=='ham']))\n",
    "print ('Spam:', len(data[data['klass']=='spam']))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing Spark session.\n",
    "spark = (\n",
    "    SparkSession.builder \n",
    "    .master(\"local\") \n",
    "    .appName(\"BIOS-823\") \n",
    "    .config(\"spark.executor.cores\", 4) \n",
    "    .getOrCreate()    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=[\"klass\",\"SMS\"]\n",
    "df = spark.createDataFrame(data, cols)\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import Tokenizer, CountVectorizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The most frequent index gets 0, so ham = 0 and spam = 1.\n",
    "indexer = StringIndexer(\n",
    "    inputCol=\"klass\", \n",
    "    outputCol=\"labels\"\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    inputCol=\"SMS\", \n",
    "    outputCol=\"SMS_tokens\")\n",
    "\n",
    "remover = StopWordsRemover(\n",
    "    inputCol=\"SMS_tokens\", \n",
    "    outputCol=\"clean_tokens\")\n",
    "\n",
    "countvectorizer=CountVectorizer(\n",
    "    inputCol=\"clean_tokens\", \n",
    "    outputCol=\"SMS_count\"\n",
    ")\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    featuresCol='SMS_count', \n",
    "    labelCol='labels',\n",
    "    maxIter=10, \n",
    "    regParam=0.001\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[indexer, tokenizer, remover, countvectorizer, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(train)\n",
    "prediction = model.transform(test)\n",
    "score = prediction.select(['labels', 'prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|labels|prediction|\n",
      "+------+----------+\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "|   0.0|       0.0|\n",
      "+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 0.972\n",
      "Precision (identifying not spam): 0.968\n",
      "Recall (identifying not spam): 0.891\n",
      "\n",
      "\n",
      "Accuracy of the model: 0.972\n",
      "Precision (identifying spam): 1.0\n",
      "Recall (identifying spam): 0.134\n"
     ]
    }
   ],
   "source": [
    "accuracy = round(score.rdd.map(lambda x: x[0] == x[1]).sum() / float(score.count()),3)\n",
    "\n",
    "#Values for identifying \"not spam\".\n",
    "tp=score.rdd.map(lambda x: (x[0] == 0) and (x[0] == 0)).sum()\n",
    "fp=score.rdd.map(lambda x: (x[0] == 1) and (x[1] == 0)).sum()\n",
    "tn=score.rdd.map(lambda x: (x[0] == 1) and (x[1] == 1)).sum()\n",
    "\n",
    "print ('Accuracy of the model:', accuracy)\n",
    "print ('Precision (identifying not spam):', round(tp/(tp+fp),3))\n",
    "print ('Recall (identifying not spam):', round(tp/(tp+tn),3))\n",
    "\n",
    "#Values for identifying \"Spam\".\n",
    "tp=score.rdd.map(lambda x: (x[0] == 1) and (x[0] == 1)).sum()\n",
    "fp=score.rdd.map(lambda x: (x[0] == 0) and (x[1] == 1)).sum()\n",
    "tn=score.rdd.map(lambda x: (x[0] == 0) and (x[1] == 0)).sum()\n",
    "\n",
    "print ('\\n')\n",
    "print ('Accuracy of the model:', accuracy)\n",
    "print ('Precision (identifying spam):', round(tp/(tp+fp),3))\n",
    "print ('Recall (identifying spam):', round(tp/(tp+tn),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2** (100 points)\n",
    "\n",
    "In this exercise, you will simulate running a machine learning pipeline to classify steaming data.\n",
    "\n",
    "- Convert the test DataFrame into a Pandas DataFrame\n",
    "- Write each row of the DataFrame to a separate tab-delimited file in a folder called \"incoming_sms\"\n",
    "- Create a Structured Streaming DataFrame using `readStream` with `option(\"maxFilesPerTrigger\", 1)` to simulate streaming data\n",
    "- Use the fitted pipeline created in Ex. 1 to transform the input stream\n",
    "- Write the transformed stream to memory with name `sms_pred\n",
    "- Sleep 30 seconds\n",
    "- Use an SQL query to show the `index`, `label` and `prediction` columns\n",
    "- Sleep 30 more seconds\n",
    "- Use an SQL query to show the `index`, `label` and `prediction` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.select(\"*\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "os.chdir('/Users/iuliia/bios-823-2019/homework/incoming_sms')\n",
    "\n",
    "for index, row in test.iterrows():\n",
    "    row = index, row['klass'], row['SMS'] #each row is index, klass label, and SMS.\n",
    "    row=pd.DataFrame(row).T\n",
    "    row.to_csv(\"%d.csv\" % index, header=False, sep='\\t')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.functions import col, split\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.\n",
    "    readStream.\n",
    "    format(\"text\").\n",
    "    option(\"maxFilesPerTrigger\", 1).\n",
    "    load('/Users/iuliia/bios-823-2019/homework/incoming_sms')\n",
    ")\n",
    "\n",
    "#Separating one column \"value\" into three new columns: index, klass, and SMS.\n",
    "df=df.withColumn('temp', split('value', '\\t')).select(*(col('temp').getItem(i).alias(f'col{i}') for i in range(4))).drop('col0')\n",
    "df = df.select(col(\"col1\").alias(\"index\"), col(\"col2\").alias(\"klass\"), col(\"col3\").alias(\"SMS\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying transformation using pipeline.\n",
    "prediction = model.transform(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    prediction.writeStream.\n",
    "    queryName(\"sms_pred\").\n",
    "    format(\"memory\").\n",
    "    outputMode(\"append\").\n",
    "    start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyspark.sql.streaming.StreamingQuery at 0x1198e5860>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+\n",
      "|index|labels|prediction|\n",
      "+-----+------+----------+\n",
      "|    6|   0.0|       0.0|\n",
      "|    7|   0.0|       0.0|\n",
      "|    5|   0.0|       0.0|\n",
      "+-----+------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----+------+----------+\n",
      "|index|labels|prediction|\n",
      "+-----+------+----------+\n",
      "|    6|   0.0|       0.0|\n",
      "|    7|   0.0|       0.0|\n",
      "|    5|   0.0|       0.0|\n",
      "|    4|   0.0|       0.0|\n",
      "|    0|   0.0|       0.0|\n",
      "|    1|   0.0|       0.0|\n",
      "+-----+------+----------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sleep(30)\n",
    "spark.sql('''SELECT index, labels, prediction FROM sms_pred''').show(3)\n",
    "sleep(30)\n",
    "spark.sql('''SELECT index, labels, prediction FROM sms_pred''').show(6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
